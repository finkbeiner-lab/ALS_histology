{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d929214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78af63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load DINO-v2 Backbone ---\n",
    "\n",
    "def get_dino_v2_backbone():\n",
    "    # This returns a model directly, not a state_dict\n",
    "    backbone = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitb14\")  # ✅ this is already a model\n",
    "    return backbone\n",
    "\n",
    "\n",
    "class DINOv2SegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.num_classes = num_classes\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(768, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        features = self.backbone.get_intermediate_layers(x, n=1)[0]  # (B, N, 768)\n",
    "        feat_size = int(features.shape[1] ** 0.5)\n",
    "        features = features.permute(0, 2, 1).reshape(B, 768, feat_size, feat_size)  # (B, 768, h, w)\n",
    "        out = self.decoder(features)  # (B, num_classes, h, w)\n",
    "        out = F.interpolate(out, size=(H, W), mode='bilinear', align_corners=False)  # upscale to 1022x1022\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "744d4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(img_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((1022, 1022)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)  # (1, 3, 1022, 1022)\n",
    "    image_np = np.array(Image.open(img_path).convert(\"RGB\").resize((1022, 1022)))\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)  # (1, num_classes, 1022, 1022)\n",
    "        pred_mask = output.argmax(dim=1).squeeze(0).cpu().numpy()  # (1022, 1022)\n",
    "        probs = torch.softmax(output, dim=1)  # convert logits → probabilities\n",
    "        conf, pred = torch.max(probs, dim=1)  # conf: confidence per pixel, pred: predicted class\n",
    "        conf = conf.squeeze(0).cpu().numpy()  # (1022, 1022)\n",
    "        pred_mask = pred.squeeze(0).cpu().numpy()  # (1022, 1022)\n",
    "        threshold = 0.6  # choose based on your model's calibration\n",
    "        high_conf_mask = np.where(conf >= threshold, pred_mask, 0)\n",
    "        #print(np.unique(high_conf_mask))\n",
    "        \n",
    "    return image_np, high_conf_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d53f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred_mask, true_mask, num_classes=2):\n",
    "    \"\"\"\n",
    "    pred_mask: torch.Tensor or np.array (H, W) — predicted class per pixel\n",
    "    true_mask: torch.Tensor or np.array (H, W) — ground truth class per pixel\n",
    "    \"\"\"\n",
    "    if isinstance(pred_mask, torch.Tensor):\n",
    "        pred_mask = pred_mask.cpu().numpy()\n",
    "    if isinstance(true_mask, torch.Tensor):\n",
    "        true_mask = true_mask.cpu().numpy()\n",
    "\n",
    "    ious = []\n",
    "    dices = []\n",
    "    pixel_acc = np.mean(pred_mask == true_mask)\n",
    "\n",
    "    for cls_1 in range(1,num_classes):\n",
    "        pred_cls = (pred_mask == cls_1)\n",
    "        true_cls = (true_mask == cls_1)\n",
    "\n",
    "        intersection = np.logical_and(pred_cls, true_cls).sum()\n",
    "        union = np.logical_or(pred_cls, true_cls).sum()\n",
    "        iou = intersection / union if union > 0 else np.nan\n",
    "\n",
    "        dice = (2 * intersection) / (pred_cls.sum() + true_cls.sum()) if (pred_cls.sum() + true_cls.sum()) > 0 else np.nan\n",
    "\n",
    "        ious.append(iou)\n",
    "        dices.append(dice)\n",
    "\n",
    "    mean_iou = np.nanmean(ious)\n",
    "    mean_dice = np.nanmean(dices)\n",
    "\n",
    "    return {\n",
    "        \"pixel_accuracy\": pixel_acc,\n",
    "        \"iou_per_class\": ious,\n",
    "        \"mean_iou\": mean_iou,\n",
    "        \"dice_per_class\": dices,\n",
    "        \"mean_dice\": mean_dice\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0002e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/gladstone/finkbeiner/steve/work/data/npsad_data/monika/ALS/dino_v2_segmentation_may30.pth\"  ## with new training Sep 12, 2025\n",
    "path = \"/gladstone/finkbeiner/steve/work/data/npsad_data/monika/ALS/dino_v2_segmentation_oct10.pth\"\n",
    "path =  \"/gladstone/finkbeiner/steve/work/data/npsad_data/monika/ALS/dino_v2_segmentation_oct17.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e566e2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mahirwar/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "xFormers not available\n",
      "xFormers not available\n",
      "/tmp/ipykernel_3170010/3586871827.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DINOv2SegmentationModel(\n",
       "  (backbone): DinoVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x NestedTensorBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MemEffAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "backbone = get_dino_v2_backbone()\n",
    "model = DINOv2SegmentationModel(backbone, num_classes=3)  # Update `num_classes` as needed\n",
    "model.load_state_dict(torch.load(path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  # 🔍 Important for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9edb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_location =  \"/gladstone/finkbeiner/steve/work/data/npsad_data/monika/ALS/all_crops/\"\n",
    "test_folders = glob(os.path.join(dataset_test_location, \"*/*/*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210f400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_all = []\n",
    "test_masks_all = []\n",
    "#for i in range(len(test_folders)):\n",
    "test_imgs = glob(os.path.join(dataset_test_location,\"*/*/image\", \"*\"))\n",
    "test_masks = glob(os.path.join(dataset_test_location,\"*/*/mask\", \"*\"))\n",
    "test_imgs_all.extend(test_imgs)\n",
    "test_masks_all.extend(test_masks)\n",
    "test_imgs_all.sort()\n",
    "test_masks_all.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166f264d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_imgs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48e02e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_masks_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b5d2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_masks(image, true_mask, pred_mask, save_path, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Overlay true and predicted segmentation masks on an RGB image.\n",
    "    \n",
    "    image: HxWx3 RGB (np.uint8 or float in [0,1])\n",
    "    true_mask: HxW (int labels, 0 = background, >0 = object id)\n",
    "    pred_mask: HxW (int labels, same format)\n",
    "    alpha: transparency for masks\n",
    "    \"\"\"\n",
    "    # Normalize image to [0,1] if needed\n",
    "    if image.dtype == np.uint8:\n",
    "        img = image.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        img = image.copy()\n",
    "\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    # Colormaps for GT and Pred\n",
    "    cmap_true = cm.get_cmap(\"Set1\", np.max(true_mask) + 1)  # distinct colors\n",
    "    cmap_pred = cm.get_cmap(\"Set2\", np.max(pred_mask) + 1)\n",
    "\n",
    "    overlay_true = np.zeros((H, W, 4))  # RGBA\n",
    "    overlay_pred = np.zeros((H, W, 4))\n",
    "\n",
    "    # Colorize true mask\n",
    "    if np.max(true_mask) > 0:\n",
    "        overlay_true = cmap_true(true_mask)  # RGBA in [0,1]\n",
    "        overlay_true[true_mask == 0] = (0,0,0,0)  # transparent bg\n",
    "    \n",
    "    # Colorize pred mask\n",
    "    if np.max(pred_mask) > 0:\n",
    "        overlay_pred = cmap_pred(pred_mask)\n",
    "        overlay_pred[pred_mask == 0] = (0,0,0,0)\n",
    "\n",
    "    # Combine: overlay GT in red tint, Pred in green tint\n",
    "    blended_true = (1 - alpha) * img + alpha * overlay_true[..., :3]\n",
    "    blended_pred = (1 - alpha) * img + alpha * overlay_pred[..., :3]\n",
    "\n",
    "    # Show side by side\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(blended_true)\n",
    "    axes[1].set_title(\"Ground Truth Mask\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(blended_pred)\n",
    "    axes[2].set_title(\"Predicted Mask\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91921ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "for img_path, mask_path in zip(test_imgs_all,test_masks_all):\n",
    "    img, pred_mask = make_prediction(img_path)\n",
    "    mask = Image.open(mask_path).convert(\"P\").resize((1022, 1022))\n",
    "    true_mask = np.array(mask)//100\n",
    "    true_ids = np.unique(true_mask)\n",
    "    print(true_ids)\n",
    "    pred_ids = np.unique(pred_mask)\n",
    "    print(pred_ids)\n",
    "    if len(pred_ids)>2:\n",
    "        all_true_masks = (true_mask[np.newaxis] == true_ids[:,  np.newaxis, np.newaxis])\n",
    "        all_pred_masks = (pred_mask[np.newaxis] == pred_ids[:,  np.newaxis, np.newaxis])\n",
    "        break\n",
    "        #save_path = \"/gladstone/finkbeiner/steve/work/data/npsad_data/monika/ALS/seg_runs/2025-10-10_09-38-20/segmentation_results/\"+img_path.split(\"/\")[-1]\n",
    "        #overlay_masks(img, all_true_masks[1], all_pred_masks[1],save_path, alpha=0.5)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e42fb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "class_maps= {1:\"glial\", 2:\"tdp43\"}\n",
    "\n",
    "def boxes_overlap(boxA, boxB):\n",
    "    xA1, yA1, xA2, yA2 = boxA\n",
    "    xB1, yB1, xB2, yB2 = boxB\n",
    "\n",
    "    # Compute overlap in each dimension\n",
    "    x_overlap = max(0, min(xA2, xB2) - max(xA1, xB1))\n",
    "    y_overlap = max(0, min(yA2, yB2) - max(yA1, yB1))\n",
    "\n",
    "    # If both overlaps are positive → boxes intersect\n",
    "    return x_overlap > 0 and y_overlap > 0\n",
    "\n",
    "total_true = 0\n",
    "total_matched = 0\n",
    "\n",
    "for img_path, mask_path in zip(test_imgs_all,test_masks_all):\n",
    "    img, pred_mask = make_prediction(img_path)\n",
    "    mask = Image.open(mask_path).convert(\"P\").resize((1022, 1022))\n",
    "    true_mask = np.array(mask)//100\n",
    "\n",
    "    class_ids = np.unique(pred_mask)\n",
    "    class_ids = class_ids[class_ids != 0]\n",
    "    true_ids = np.unique(true_mask)\n",
    "    true_ids= true_ids[true_ids != 0]\n",
    "    \n",
    "    true_boxes = []\n",
    "    pred_boxes = []\n",
    "    count = {'gt':{'glial':0, 'tdp43':0},'pred':{'glial':0, 'tdp43':0}}\n",
    "    #color_mask = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    for true_id in true_ids:\n",
    "        binary = (true_mask == true_id).astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        sorted_with_area = sorted(\n",
    "        [(c, cv2.contourArea(c)) for c in contours],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True)\n",
    "        \n",
    "        for cnt, area in sorted_with_area:\n",
    "            if area>2000:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 0), 3)\n",
    "                cv2.putText(img, f\"true: {class_maps[true_id]}\", (x, y - 5),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "            \n",
    "                true_boxes.append([x, y,x + w, y + h])\n",
    "                print(true_id)\n",
    "                if true_id==1:\n",
    "                    count['gt']['glial']+=1\n",
    "                if true_id==2:\n",
    "                    count['gt']['tdp43']+=1\n",
    "    # Draw bounding boxes\n",
    "    for class_id in class_ids:\n",
    "        # Create binary mask for this class\n",
    "        binary = (pred_mask == class_id).astype(np.uint8)\n",
    "\n",
    "        # Find contours for that class\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        #contours = list(map(lambda t: t[0], sorted([(contour, cv2.contourArea(contour)) for contour in contours], key=lambda t: -t[1])))\n",
    "\n",
    "        sorted_with_area = sorted(\n",
    "        [(c, cv2.contourArea(c)) for c in contours],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "        \n",
    "    )\n",
    "        \n",
    "        # Draw bounding box for each contour\n",
    "        for contour, area in sorted_with_area:\n",
    "            if area>2000:\n",
    "                #print(area)\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                pred_boxes.append([x, y,x + w, y + h])\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 4)\n",
    "                cv2.putText(img, f\"pred: {class_maps[class_id]}\", (x, y - 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                if class_id==1:\n",
    "                    count['pred']['glial']+=1\n",
    "                if class_id==2:\n",
    "                    count['pred']['tdp43']+=1\n",
    "                \n",
    "    save_path = \"/gladstone/finkbeiner/steve/work/data/npsad_data/monika/ALS/seg_runs/2025-10-10_09-38-20/segmentation_bbox_formatted/\"+img_path.split(\"/\")[-1]\n",
    "\n",
    "    # Display the result\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Ground truth and prediction bounding boxes\")\n",
    "    plt.axis('off')\n",
    "    plt.figtext(0.5, 0.02, 'GT: [glial-'+str(count['gt']['glial'])+ \",tdp43- \"+str(count['gt']['tdp43'])+\"]\"\n",
    "                 '; pred: [glial-'+str(count['pred']['glial'])+ \", tdp43- \"+str(count['pred']['tdp43'])+\"]\",\n",
    "            horizontalalignment='center',  # Center the text horizontally\n",
    "            fontsize=10,\n",
    "            color='red')    \n",
    "    #plt.show()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    total_true = total_true + len(true_boxes)\n",
    "    matched = 0\n",
    "    for true_box in true_boxes:\n",
    "        for pred_box in pred_boxes:\n",
    "            intersect = boxes_overlap(true_box, pred_box)\n",
    "            if intersect==True:\n",
    "                matched=matched+1\n",
    "    \n",
    "    total_matched  =  total_matched + matched\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33efa743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e77a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6370d6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04e58630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8135168961201502"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_matched/total_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bad13fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7683089214380826"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_matched/total_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ab7d98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8135168961201502"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_matched/total_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1de1cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kfold_amy_plaque1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
